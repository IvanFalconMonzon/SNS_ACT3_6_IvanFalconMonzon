{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Iván Falcón Monzón**"
      ],
      "metadata": {
        "id": "VJ1JrSGVpth9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actividad 3.6 - DengAI: predicción de la propagación de enfermedades"
      ],
      "metadata": {
        "id": "gFuRF4t1pwad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Archivos de la competición**\n",
        "\n",
        "dengue_features_train.csv → Características de entrenamiento.\n",
        "dengue_labels_train.csv → Etiquetas (casos totales de dengue).\n",
        "dengue_features_test.csv → Características de prueba (para predicción).\n",
        "submission_format.csv → Formato para subir el archivo de predicciones."
      ],
      "metadata": {
        "id": "iM5FTZvMpnYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificar versiones complatibles con el código:\n"
      ],
      "metadata": {
        "id": "Ye8X83qywBm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import xgboost\n",
        "\n",
        "print(\"Scikit-learn version:\", sklearn.__version__)\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "print(\"XGBoost version:\", xgboost.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z_DqzzkwFVn",
        "outputId": "9c03abcc-483f-4a99-b56d-d4b69c99c50c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scikit-learn version: 1.3.0\n",
            "NumPy version: 1.23.5\n",
            "XGBoost version: 2.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-learn version: 1.3.0\n",
        "\n",
        "NumPy version: 1.23.5\n",
        "\n",
        "XGBoost version: 2.0.3"
      ],
      "metadata": {
        "id": "Xe45gZ-KwE4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "He tenido problemas con los modelos y las versiones, asique si no funciona el código instalar las nuevas versiones."
      ],
      "metadata": {
        "id": "0lanrb0UwX6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y scikit-learn numpy\n",
        "#!pip install --no-cache-dir numpy==1.23.5 scikit-learn==1.3.0\n",
        "#!pip install --force-reinstall xgboost==2.0.3"
      ],
      "metadata": {
        "id": "WHvkbPsHwcA6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cargar los datos**"
      ],
      "metadata": {
        "id": "p4ZOwgg_pyzt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3u-XWd7oqt7",
        "outputId": "4cb7aa61-0188-43a4-abf4-0e5df1e584af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  city  year  weekofyear week_start_date   ndvi_ne   ndvi_nw   ndvi_se  \\\n",
            "0   sj  1990          18      1990-04-30  0.122600  0.103725  0.198483   \n",
            "1   sj  1990          19      1990-05-07  0.169900  0.142175  0.162357   \n",
            "2   sj  1990          20      1990-05-14  0.032250  0.172967  0.157200   \n",
            "3   sj  1990          21      1990-05-21  0.128633  0.245067  0.227557   \n",
            "4   sj  1990          22      1990-05-28  0.196200  0.262200  0.251200   \n",
            "\n",
            "    ndvi_sw  precipitation_amt_mm  reanalysis_air_temp_k  ...  \\\n",
            "0  0.177617                 12.42             297.572857  ...   \n",
            "1  0.155486                 22.82             298.211429  ...   \n",
            "2  0.170843                 34.54             298.781429  ...   \n",
            "3  0.235886                 15.36             298.987143  ...   \n",
            "4  0.247340                  7.52             299.518571  ...   \n",
            "\n",
            "   reanalysis_precip_amt_kg_per_m2  reanalysis_relative_humidity_percent  \\\n",
            "0                            32.00                             73.365714   \n",
            "1                            17.94                             77.368571   \n",
            "2                            26.10                             82.052857   \n",
            "3                            13.90                             80.337143   \n",
            "4                            12.20                             80.460000   \n",
            "\n",
            "   reanalysis_sat_precip_amt_mm  reanalysis_specific_humidity_g_per_kg  \\\n",
            "0                         12.42                              14.012857   \n",
            "1                         22.82                              15.372857   \n",
            "2                         34.54                              16.848571   \n",
            "3                         15.36                              16.672857   \n",
            "4                          7.52                              17.210000   \n",
            "\n",
            "   reanalysis_tdtr_k  station_avg_temp_c  station_diur_temp_rng_c  \\\n",
            "0           2.628571           25.442857                 6.900000   \n",
            "1           2.371429           26.714286                 6.371429   \n",
            "2           2.300000           26.714286                 6.485714   \n",
            "3           2.428571           27.471429                 6.771429   \n",
            "4           3.014286           28.942857                 9.371429   \n",
            "\n",
            "   station_max_temp_c  station_min_temp_c  station_precip_mm  \n",
            "0                29.4                20.0               16.0  \n",
            "1                31.7                22.2                8.6  \n",
            "2                32.2                22.8               41.4  \n",
            "3                33.3                23.3                4.0  \n",
            "4                35.0                23.9                5.8  \n",
            "\n",
            "[5 rows x 24 columns]\n",
            "  city  year  weekofyear  total_cases\n",
            "0   sj  1990          18            4\n",
            "1   sj  1990          19            5\n",
            "2   sj  1990          20            4\n",
            "3   sj  1990          21            3\n",
            "4   sj  1990          22            6\n",
            "  city  year  weekofyear week_start_date  ndvi_ne   ndvi_nw   ndvi_se  \\\n",
            "0   sj  2008          18      2008-04-29  -0.0189 -0.018900  0.102729   \n",
            "1   sj  2008          19      2008-05-06  -0.0180 -0.012400  0.082043   \n",
            "2   sj  2008          20      2008-05-13  -0.0015       NaN  0.151083   \n",
            "3   sj  2008          21      2008-05-20      NaN -0.019867  0.124329   \n",
            "4   sj  2008          22      2008-05-27   0.0568  0.039833  0.062267   \n",
            "\n",
            "    ndvi_sw  precipitation_amt_mm  reanalysis_air_temp_k  ...  \\\n",
            "0  0.091200                 78.60             298.492857  ...   \n",
            "1  0.072314                 12.56             298.475714  ...   \n",
            "2  0.091529                  3.66             299.455714  ...   \n",
            "3  0.125686                  0.00             299.690000  ...   \n",
            "4  0.075914                  0.76             299.780000  ...   \n",
            "\n",
            "   reanalysis_precip_amt_kg_per_m2  reanalysis_relative_humidity_percent  \\\n",
            "0                            25.37                             78.781429   \n",
            "1                            21.83                             78.230000   \n",
            "2                             4.12                             78.270000   \n",
            "3                             2.20                             73.015714   \n",
            "4                             4.36                             74.084286   \n",
            "\n",
            "   reanalysis_sat_precip_amt_mm  reanalysis_specific_humidity_g_per_kg  \\\n",
            "0                         78.60                              15.918571   \n",
            "1                         12.56                              15.791429   \n",
            "2                          3.66                              16.674286   \n",
            "3                          0.00                              15.775714   \n",
            "4                          0.76                              16.137143   \n",
            "\n",
            "   reanalysis_tdtr_k  station_avg_temp_c  station_diur_temp_rng_c  \\\n",
            "0           3.128571           26.528571                 7.057143   \n",
            "1           2.571429           26.071429                 5.557143   \n",
            "2           4.428571           27.928571                 7.785714   \n",
            "3           4.342857           28.057143                 6.271429   \n",
            "4           3.542857           27.614286                 7.085714   \n",
            "\n",
            "   station_max_temp_c  station_min_temp_c  station_precip_mm  \n",
            "0                33.3                21.7               75.2  \n",
            "1                30.0                22.2               34.3  \n",
            "2                32.8                22.8                3.0  \n",
            "3                33.3                24.4                0.3  \n",
            "4                33.3                23.3               84.1  \n",
            "\n",
            "[5 rows x 24 columns]\n"
          ]
        }
      ],
      "source": [
        "# IVAN FALCON MONZON\n",
        "import pandas as pd\n",
        "\n",
        "# Enlaces directos a los archivos CSV en GitHub (debes reemplazarlos con los reales)\n",
        "github_base = \"https://raw.githubusercontent.com/IvanFalconMonzon/SNS_ACT3_6_IvanFalconMonzon/main/\"\n",
        "\n",
        "# Cargar datasets desde GitHub\n",
        "train_features = pd.read_csv(github_base + \"dengue_features_train.csv\")\n",
        "train_labels = pd.read_csv(github_base + \"dengue_labels_train.csv\")\n",
        "test_features = pd.read_csv(github_base + \"dengue_features_test.csv\")\n",
        "\n",
        "# Verificar estructura de los datasets\n",
        "print(train_features.head())\n",
        "print(train_labels.head())\n",
        "print(test_features.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unir las etiquetas con los datos de entrenamiento**"
      ],
      "metadata": {
        "id": "QLuHSuXaqN7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "# Unir las etiquetas con las características (clave: 'city', 'year', 'weekofyear')\n",
        "train_data = train_features.merge(train_labels, on=['city', 'year', 'weekofyear'])\n",
        "\n",
        "# Separar características (X) y variable objetivo (y)\n",
        "X = train_data.drop(columns=['city', 'year', 'weekofyear', 'total_cases'])\n",
        "y = train_data['total_cases']\n",
        "X_test = test_features.drop(columns=['city', 'year', 'weekofyear'])"
      ],
      "metadata": {
        "id": "3Gx-VxCwqLc9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocesamiento**"
      ],
      "metadata": {
        "id": "eUTqZHNiqRAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Rellenar valores nulos (forward fill):\n"
      ],
      "metadata": {
        "id": "yg3r0hlCqUMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "X = X.fillna(method='ffill')\n",
        "X_test = X_test.fillna(method='ffill')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "537s-WVkqRx9",
        "outputId": "a11ac840-191e-4573-f891-25100bb05fb2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-a046242418a9>:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X = X.fillna(method='ffill')\n",
            "<ipython-input-5-a046242418a9>:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_test = X_test.fillna(method='ffill')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. División de datos en entrenamiento y validación\n"
      ],
      "metadata": {
        "id": "xOQvym-jqfZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "from sklearn.model_selection import train_test_split  # Importa la función para dividir los datos\n",
        "\n",
        "# Divide el conjunto de datos en entrenamiento (80%) y validación (20%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,               # X: características, y: variable objetivo\n",
        "    test_size=0.2,      # 20% de los datos se usarán para validación\n",
        "    random_state=42     # Fija una semilla para obtener siempre la misma división\n",
        ")"
      ],
      "metadata": {
        "id": "GPyEBBIjqfq1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversión de fechas a valores numéricos"
      ],
      "metadata": {
        "id": "TCYIaIOjrOS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "# Convertir la columna 'week_start_date' a tipo datetime\n",
        "X['week_start_date'] = pd.to_datetime(X['week_start_date'])\n",
        "X_test['week_start_date'] = pd.to_datetime(X_test['week_start_date'])\n",
        "\n",
        "# Definir una fecha base para calcular los días transcurridos\n",
        "base_date = pd.to_datetime(\"1990-01-01\")\n",
        "\n",
        "# Crear una nueva columna con la cantidad de días desde la fecha base\n",
        "X['days_since_start'] = (X['week_start_date'] - base_date).dt.days\n",
        "X_test['days_since_start'] = (X_test['week_start_date'] - base_date).dt.days\n",
        "\n",
        "# Eliminar la columna original de fecha, ya que ahora tenemos una versión numérica\n",
        "X = X.drop(columns=['week_start_date'])\n",
        "X_test = X_test.drop(columns=['week_start_date'])"
      ],
      "metadata": {
        "id": "C6uuUKyvrIO0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminación segura de la columna week_start_date"
      ],
      "metadata": {
        "id": "vBt5i9iHxGuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "# Eliminar la columna 'week_start_date' si existe, evitando errores si ya fue eliminada\n",
        "X_train = X_train.drop(columns=['week_start_date'], errors='ignore')\n",
        "X_val = X_val.drop(columns=['week_start_date'], errors='ignore')\n",
        "X_test = X_test.drop(columns=['week_start_date'], errors='ignore')"
      ],
      "metadata": {
        "id": "ecgJN5kxryOU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Conversión de la fecha 'week_start_date' a formato datetime y extracción de componentes"
      ],
      "metadata": {
        "id": "wMYH6B__xl96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "# Convertir 'week_start_date' a formato datetime y extraer Año, Mes, Día en TODOS los conjuntos\n",
        "for df in [X_train, X_val, X_test]:  # Iterar sobre cada uno de los conjuntos de datos (entrenamiento, validación y prueba)\n",
        "    if 'week_start_date' in df.columns:  # Comprobar si la columna 'week_start_date' existe en el DataFrame\n",
        "        # Convertir la columna 'week_start_date' a tipo datetime (para manipulación de fechas)\n",
        "        df['week_start_date'] = pd.to_datetime(df['week_start_date'])\n",
        "\n",
        "        # Extraer el año de la columna 'week_start_date' y guardarlo en una nueva columna 'year'\n",
        "        df['year'] = df['week_start_date'].dt.year\n",
        "\n",
        "        # Extraer el mes de la columna 'week_start_date' y guardarlo en una nueva columna 'month'\n",
        "        df['month'] = df['week_start_date'].dt.month\n",
        "\n",
        "        # Extraer el día de la columna 'week_start_date' y guardarlo en una nueva columna 'day'\n",
        "        df['day'] = df['week_start_date'].dt.day\n",
        "\n",
        "        # Eliminar la columna original 'week_start_date' ya que la información ahora está separada en nuevas columnas\n",
        "        df.drop(columns=['week_start_date'], inplace=True)"
      ],
      "metadata": {
        "id": "Ru8uzfN_2y5i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Entrenar modelos**"
      ],
      "metadata": {
        "id": "4IlKyw2hqqAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo 1: Random Forest**"
      ],
      "metadata": {
        "id": "Rg2zqlzZqrEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificar si hay valores de tipo string en los datos"
      ],
      "metadata": {
        "id": "7lYVGWbLsJcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "print(\"Columnas en X_train:\", X_train.columns)\n",
        "print(\"Columnas en X_val:\", X_val.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wQ2DWYYrZGU",
        "outputId": "7d0af3e4-666a-4673-b70e-cbac49221f7f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columnas en X_train: Index(['ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw', 'precipitation_amt_mm',\n",
            "       'reanalysis_air_temp_k', 'reanalysis_avg_temp_k',\n",
            "       'reanalysis_dew_point_temp_k', 'reanalysis_max_air_temp_k',\n",
            "       'reanalysis_min_air_temp_k', 'reanalysis_precip_amt_kg_per_m2',\n",
            "       'reanalysis_relative_humidity_percent', 'reanalysis_sat_precip_amt_mm',\n",
            "       'reanalysis_specific_humidity_g_per_kg', 'reanalysis_tdtr_k',\n",
            "       'station_avg_temp_c', 'station_diur_temp_rng_c', 'station_max_temp_c',\n",
            "       'station_min_temp_c', 'station_precip_mm'],\n",
            "      dtype='object')\n",
            "Columnas en X_val: Index(['ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw', 'precipitation_amt_mm',\n",
            "       'reanalysis_air_temp_k', 'reanalysis_avg_temp_k',\n",
            "       'reanalysis_dew_point_temp_k', 'reanalysis_max_air_temp_k',\n",
            "       'reanalysis_min_air_temp_k', 'reanalysis_precip_amt_kg_per_m2',\n",
            "       'reanalysis_relative_humidity_percent', 'reanalysis_sat_precip_amt_mm',\n",
            "       'reanalysis_specific_humidity_g_per_kg', 'reanalysis_tdtr_k',\n",
            "       'station_avg_temp_c', 'station_diur_temp_rng_c', 'station_max_temp_c',\n",
            "       'station_min_temp_c', 'station_precip_mm'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento y Evaluación de un Modelo Random Forest**"
      ],
      "metadata": {
        "id": "uEOVDj5osP1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "\n",
        "# Importar el modelo RandomForestRegressor de la librería sklearn\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Importar la función mean_absolute_error para calcular el error absoluto medio\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Crear una instancia del modelo RandomForestRegressor con 100 árboles (n_estimators) y una semilla para la aleatoriedad (random_state)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "# Ajustar el modelo a los datos de entrenamiento (X_train y y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Realizar predicciones sobre el conjunto de validación (X_val)\n",
        "y_pred_rf = rf.predict(X_val)\n",
        "\n",
        "# Calcular el error absoluto medio (MAE) comparando las predicciones con los valores reales del conjunto de validación (y_val)\n",
        "mae_rf = mean_absolute_error(y_val, y_pred_rf)\n",
        "\n",
        "# Mostrar el resultado del MAE para el modelo Random Forest\n",
        "print(f\"MAE - Random Forest: {mae_rf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACDVbzOLqtHN",
        "outputId": "3adeec52-61fd-4422-afe3-134053ffae34"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE - Random Forest: 19.546690639269407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento y Evaluación de un Modelo XGBoost**"
      ],
      "metadata": {
        "id": "2Wf0XEqj357a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "\n",
        "# Importar el modelo XGBRegressor de la librería xgboost\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Crear una instancia del modelo XGBRegressor con 100 estimadores, tasa de aprendizaje de 0.1 y semilla para la aleatoriedad\n",
        "xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Ajustar el modelo a los datos de entrenamiento (X_train y y_train)\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Realizar predicciones sobre el conjunto de validación (X_val)\n",
        "y_pred_xgb = xgb.predict(X_val)\n",
        "\n",
        "# Calcular el error absoluto medio (MAE) comparando las predicciones con los valores reales del conjunto de validación (y_val)\n",
        "mae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n",
        "\n",
        "# Mostrar el resultado del MAE para el modelo XGBoost\n",
        "print(f\"MAE - XGBoost: {mae_xgb}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CubDNEhV35Q-",
        "outputId": "e48fecc0-e916-4603-d533-cf9882684909"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE - XGBoost: 19.133190783123446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento y Evaluación de un Modelo de Regresión Lineal**"
      ],
      "metadata": {
        "id": "T5Qaxmb6ybWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "\n",
        "# Importar el modelo LinearRegression de la librería sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Crear una instancia del modelo LinearRegression\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Ajustar el modelo a los datos de entrenamiento (X_train y y_train)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Realizar predicciones sobre el conjunto de validación (X_val)\n",
        "y_pred_lr = lr.predict(X_val)\n",
        "\n",
        "# Calcular el error absoluto medio (MAE) comparando las predicciones con los valores reales del conjunto de validación (y_val)\n",
        "mae_lr = mean_absolute_error(y_val, y_pred_lr)\n",
        "\n",
        "# Mostrar el resultado del MAE para el modelo de regresión lineal\n",
        "print(f\"MAE - Regresión Lineal: {mae_lr}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tybH1liX38Hp",
        "outputId": "8d179dc4-f60f-4550-acc2-b92e77045235"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE - Regresión Lineal: 25.343064369691845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Búsqueda de Hiperparámetros Óptimos con GridSearchCV**"
      ],
      "metadata": {
        "id": "tblJrxLt0WI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "\n",
        "# Importar GridSearchCV desde la librería sklearn.model_selection para la búsqueda de hiperparámetros\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Definir la cuadrícula de parámetros a evaluar (n_estimators, max_depth, min_samples_split)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Crear una instancia de GridSearchCV, aplicando RandomForestRegressor y los parámetros definidos, con validación cruzada de 3 pliegues (cv=3)\n",
        "# Se utiliza 'neg_mean_absolute_error' como la métrica para la evaluación\n",
        "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=3, scoring='neg_mean_absolute_error')\n",
        "\n",
        "# Ajustar el GridSearchCV a los datos de entrenamiento (X_train, y_train)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Imprimir los mejores parámetros encontrados\n",
        "print(\"Mejores parámetros:\", grid_search.best_params_)\n",
        "\n",
        "# Imprimir el mejor MAE (negativo, por eso se multiplica por -1)\n",
        "print(\"Mejor MAE:\", -grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUm_p7Vh3-kh",
        "outputId": "9fda4c5a-a8b7-428a-ff44-05f59796d648"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores parámetros: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Mejor MAE: 16.638303605414137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Búsqueda Aleatoria de Hiperparámetros con RandomizedSearchCV**"
      ],
      "metadata": {
        "id": "AMikpfJkrRVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "\n",
        "# Importar RandomizedSearchCV desde la librería sklearn.model_selection para la búsqueda aleatoria de hiperparámetros\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Importar XGBRegressor de la librería xgboost\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Definir la distribución de parámetros a explorar (n_estimators, learning_rate, max_depth)\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 6, 10]\n",
        "}\n",
        "\n",
        "# Inicializar el modelo XGBRegressor antes de pasarlo a RandomizedSearchCV\n",
        "xgb_regressor = XGBRegressor()\n",
        "\n",
        "# Crear una instancia de RandomizedSearchCV, con 10 iteraciones (n_iter=10), validación cruzada de 3 pliegues (cv=3)\n",
        "# Se utiliza 'neg_mean_absolute_error' como la métrica de evaluación\n",
        "random_search = RandomizedSearchCV(\n",
        "    xgb_regressor,  # Usar la instancia de XGBRegressor inicializada\n",
        "    param_dist,\n",
        "    n_iter=10,  # Número de combinaciones aleatorias a probar\n",
        "    cv=3,  # Número de pliegues en la validación cruzada\n",
        "    scoring='neg_mean_absolute_error',  # Usar el error absoluto medio negativo como la métrica de evaluación\n",
        "    random_state=42,  # Semilla para la aleatoriedad\n",
        "    n_jobs=1  # Usar un solo hilo para evitar problemas con múltiples hilos\n",
        ")\n",
        "\n",
        "# Ajustar el RandomizedSearchCV a los datos de entrenamiento (X_train, y_train)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Imprimir los mejores parámetros encontrados durante la búsqueda\n",
        "print(\"Mejores parámetros:\", random_search.best_params_)\n",
        "\n",
        "# Imprimir el mejor MAE (negativo, por eso se multiplica por -1)\n",
        "print(\"Mejor MAE:\", -random_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80yHLy9G4BH6",
        "outputId": "4879cd86-573c-4c07-81c2-e3cbcb36c695"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores parámetros: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.1}\n",
            "Mejor MAE: 16.614883451271304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alinear las columnas de X_test con X_train**"
      ],
      "metadata": {
        "id": "vCdqW4mH09fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "\n",
        "# Asegurar que las columnas de X_test estén alineadas con las columnas de X_train\n",
        "# Esto es útil si ha habido cambios en las columnas de X_train (por ejemplo, después de la selección de características) y queremos que X_test tenga las mismas columnas\n",
        "X_test = X_test[X_train.columns]"
      ],
      "metadata": {
        "id": "ObZv7fEW9-qO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cargar y Guardar Formato de Submission con Predicciones**"
      ],
      "metadata": {
        "id": "VCDAsVvZ1Ds1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Enlace directo al archivo en GitHub (verifica que sea el correcto)\n",
        "github_base = \"https://raw.githubusercontent.com/IvanFalconMonzon/SNS_ACT3_6_IvanFalconMonzon/main/\"\n",
        "\n",
        "# Cargar el formato de submission desde GitHub (archivo CSV que contiene la estructura esperada)\n",
        "submission = pd.read_csv(github_base + \"submission_format.csv\")\n",
        "\n",
        "# Get predictions from your best model or any trained model\n",
        "# For example, if 'random_search' is your best model:\n",
        "predictions = random_search.best_estimator_.predict(X_test)\n",
        "# Or if you want to use XGBoost:\n",
        "# predictions = model_xgb.predict(X_test) # Make sure X_test has the same columns and preprocessing as your training data\n",
        "# Or if you want to use the RandomForest:\n",
        "# predictions = model_rf.predict(X_test) # Make sure X_test has the same columns and preprocessing as your training data\n",
        "\n",
        "# Assign predictions to the 'total_cases' column, rounding and converting to integers\n",
        "submission['total_cases'] = predictions.round().astype(int)\n",
        "\n",
        "# Guardar el archivo CSV en Colab\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "# Descargar el archivo automáticamente en la carpeta de \"Descargas\" de tu equipo\n",
        "files.download(\"submission.csv\")\n",
        "\n",
        "# Imprimir mensaje de confirmación\n",
        "print(\"El archivo submission.csv se ha descargado automáticamente.\")"
      ],
      "metadata": {
        "id": "Qu5E73dO4DHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0062928-54dd-4b3f-a62c-eeadd47dfa98"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aabc957a-7989-469c-a5a9-6ea9dd8b5620\", \"submission.csv\", 5675)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El archivo submission.csv se ha descargado automáticamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusiones de los Resultados:**"
      ],
      "metadata": {
        "id": "KX8anqui4GWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Comparación de Modelos Básicos:\n",
        "\n",
        "* **Random Forest**: El modelo de Random Forest tiene un MAE de 19.55, lo que indica un rendimiento razonable pero no el mejor.\n",
        "* **XGBoost**: El modelo de XGBoost tiene un MAE de 19.13, lo que lo coloca ligeramente por encima del Random Forest. Esto sugiere que XGBoost podría estar manejando mejor las características y las interacciones en los datos que Random Forest.\n",
        "* **Regresión** **Lineal**: El modelo de Regresión Lineal tiene un MAE de 25.34, lo que indica que este modelo es menos efectivo en este caso. La regresión lineal es probablemente demasiado simple para capturar las relaciones complejas en los datos."
      ],
      "metadata": {
        "id": "48xwvYtP4Q6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Optimización de Hiperparámetros:\n",
        "\n",
        "* **GridSearchCV** (Búsqueda Exhaustiva): La búsqueda de los mejores hiperparámetros con GridSearchCV resultó en un MAE de 16.65, lo que es una mejora notable sobre los modelos básicos. Los mejores parámetros fueron:\n",
        " * max_depth = 10\n",
        " * min_samples_split = 2\n",
        " * n_estimators = 200 Esto muestra que la optimización de los parámetros tiene un impacto positivo en el rendimiento del modelo.\n",
        "\n",
        "* **RandomizedSearchCV** (Búsqueda Aleatoria): La búsqueda de hiperparámetros con RandomizedSearchCV dio como resultado un MAE de 16.61, que es aún más bajo que el de GridSearchCV. Los mejores parámetros fueron:\n",
        " * n_estimators = 50\n",
        " * max_depth = 3\n",
        " * learning_rate = 0.1 Este método también encontró un conjunto de parámetros efectivos, aunque no fue tan exhaustivo como GridSearchCV."
      ],
      "metadata": {
        "id": "ugyz3d5L4asK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumen:\n",
        "\n",
        "* XGBoost y Random Forest son los modelos más prometedores, con XGBoost teniendo un rendimiento ligeramente superior.\n",
        "* La Regresión Lineal no es tan efectiva en este caso, ya que su MAE es más alto.\n",
        "* Las búsquedas de hiperparámetros (tanto con GridSearchCV como RandomizedSearchCV) mejoraron significativamente el rendimiento, con RandomizedSearchCV mostrando el mejor MAE.\n",
        "* Las optimizaciones sugieren que el modelo final con los parámetros óptimos puede mejorar considerablemente el rendimiento en comparación con los modelos básicos."
      ],
      "metadata": {
        "id": "jtGwJwc_4tUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **REPOSITORIOS**"
      ],
      "metadata": {
        "id": "aL6yOp896oP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colab: https://colab.research.google.com/drive/1Jy5ZJmDnXHav2Co6hypYFYCfFjD8T3tF?usp=sharing\n",
        "\n",
        "Github: https://github.com/IvanFalconMonzon/SNS_ACT3_6_IvanFalconMonzon.git"
      ],
      "metadata": {
        "id": "TvMNkPNF6qjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BUSCANDO MEJORAS**"
      ],
      "metadata": {
        "id": "ikTJHsuq6mQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Cargar datasets\n",
        "github_base = \"https://raw.githubusercontent.com/IvanFalconMonzon/SNS_ACT3_6_IvanFalconMonzon/main/\"\n",
        "train_features = pd.read_csv(github_base + \"dengue_features_train.csv\")\n",
        "train_labels = pd.read_csv(github_base + \"dengue_labels_train.csv\")\n",
        "test_features = pd.read_csv(github_base + \"dengue_features_test.csv\")\n",
        "\n",
        "# Identificar columnas no numéricas\n",
        "non_numeric_columns = train_features.select_dtypes(include=['object']).columns\n",
        "print(f\"Columnas con valores no numéricos: {non_numeric_columns}\")\n",
        "\n",
        "# Transformar 'week_start_date' a características numéricas\n",
        "train_features['week_start_date'] = pd.to_datetime(train_features['week_start_date'])\n",
        "train_features['year'] = train_features['week_start_date'].dt.year\n",
        "train_features['month'] = train_features['week_start_date'].dt.month\n",
        "train_features['day'] = train_features['week_start_date'].dt.day\n",
        "train_features.drop(columns=['week_start_date'], inplace=True)\n",
        "\n",
        "# Eliminar la columna 'city' o aplicar OneHotEncoding\n",
        "train_features = pd.get_dummies(train_features, columns=['city'], drop_first=True)\n",
        "\n",
        "# Limpiar 'y_train' eliminando o corrigiendo valores no numéricos\n",
        "train_labels = pd.to_numeric(train_labels['total_cases'], errors='coerce')  # Convertir a NaN los valores no numéricos\n",
        "train_labels = train_labels.dropna()  # Eliminar filas con valores NaN\n",
        "\n",
        "# Asegurarnos de que train_features y train_labels tengan el mismo número de muestras\n",
        "train_features = train_features.iloc[:len(train_labels), :]\n",
        "\n",
        "# Imputar los valores NaN con la media de cada columna\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "train_features_imputed = imputer.fit_transform(train_features)\n",
        "\n",
        "# Dividir en datos de entrenamiento y validación\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_features_imputed, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Entrenamiento y predicción con RandomForestRegressor\n",
        "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model_rf.fit(X_train_scaled, y_train)\n",
        "y_pred_rf = model_rf.predict(X_val_scaled)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mae_rf = mean_absolute_error(y_val, y_pred_rf)\n",
        "print(f\"MAE - Random Forest después de limpiar y transformar datos: {mae_rf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-BtI9ZQ59PL",
        "outputId": "fd5ad1a1-3147-40ec-d616-799bbe43b9b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columnas con valores no numéricos: Index(['city', 'week_start_date'], dtype='object')\n",
            "MAE - Random Forest después de limpiar y transformar datos: 14.61359589041096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " GridSearchCV: Búsqueda exhaustiva sobre un conjunto de parámetros predefinidos"
      ],
      "metadata": {
        "id": "5OAJFIu87Aqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Definir el rango de hiperparámetros que quieres probar\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],        # Número de árboles en el bosque\n",
        "    'max_depth': [10, 20, None],            # Profundidad máxima de los árboles\n",
        "    'min_samples_split': [2, 5, 10],        # Mínimo número de muestras requeridas para dividir un nodo\n",
        "    'min_samples_leaf': [1, 2, 4],          # Mínimo número de muestras en una hoja\n",
        "    'bootstrap': [True, False],             # Si se debe usar muestreo con reemplazo\n",
        "}\n",
        "\n",
        "# Crear el modelo de RandomForest\n",
        "model_rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros\n",
        "grid_search = GridSearchCV(estimator=model_rf, param_grid=param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Mostrar los mejores parámetros y el mejor MAE encontrado\n",
        "print(\"Mejores parámetros:\", grid_search.best_params_)\n",
        "print(\"Mejor MAE:\", -grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l618lbGY63dL",
        "outputId": "cd94f0e0-b38a-4868-f0c5-452666f0c5d2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores parámetros: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Mejor MAE: 13.499927798467558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomizedSearchCV: Búsqueda aleatoria de hiperparámetros"
      ],
      "metadata": {
        "id": "iUGyOAfS7AIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "\n",
        "# Definir el rango de hiperparámetros que quieres probar\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300],         # Número de árboles en el bosque\n",
        "    'max_depth': [10, 20, 30, None],               # Profundidad máxima de los árboles\n",
        "    'min_samples_split': [2, 5, 10],               # Mínimo número de muestras requeridas para dividir un nodo\n",
        "    'min_samples_leaf': [1, 2, 4],                 # Mínimo número de muestras en una hoja\n",
        "    'bootstrap': [True, False],                    # Si se debe usar muestreo con reemplazo\n",
        "}\n",
        "\n",
        "# Crear el modelo de RandomForest\n",
        "model_rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros con una cantidad limitada de combinaciones aleatorias\n",
        "random_search = RandomizedSearchCV(estimator=model_rf, param_distributions=param_dist, n_iter=10, cv=3,\n",
        "                                   scoring='neg_mean_absolute_error', random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Mostrar los mejores parámetros y el mejor MAE encontrado\n",
        "print(\"Mejores parámetros:\", random_search.best_params_)\n",
        "print(\"Mejor MAE:\", -random_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICnDnnYx64Z0",
        "outputId": "a9ef6268-f9f0-4463-ed5b-e4713645a44c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores parámetros: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 20, 'bootstrap': True}\n",
            "Mejor MAE: 13.862199448481727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluación de los resultados"
      ],
      "metadata": {
        "id": "jG90uPmx6-Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "# Usar los mejores parámetros encontrados en GridSearchCV o RandomizedSearchCV\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Entrenamiento y predicción con el mejor modelo\n",
        "best_model.fit(X_train_scaled, y_train)\n",
        "y_pred_best = best_model.predict(X_val_scaled)\n",
        "\n",
        "# Evaluación del modelo\n",
        "mae_best = mean_absolute_error(y_val, y_pred_best)\n",
        "print(f\"Mejor MAE después de optimizar hiperparámetros: {mae_best}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlodDa7p66_i",
        "outputId": "ffaebba6-5a6c-4c47-e98a-a3190ecaaed4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejor MAE después de optimizar hiperparámetros: 14.751873705416706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementación de XGBoost:"
      ],
      "metadata": {
        "id": "IGVXdLNYAUMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Entrenamiento de XGBoost\n",
        "model_xgb = xgb.XGBRegressor(n_estimators=200, max_depth=10, learning_rate=0.1, random_state=42)\n",
        "model_xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predicciones\n",
        "y_pred_xgb = model_xgb.predict(X_val_scaled)\n",
        "\n",
        "# Evaluación\n",
        "mae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n",
        "print(\"MAE - XGBoost: \", mae_xgb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXUHmuTjAUth",
        "outputId": "cf917505-cc20-45a1-a7eb-fa5768db07f9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE - XGBoost:  14.37872043448462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementación con Optuna:**"
      ],
      "metadata": {
        "id": "onTaaaTHAadM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optuna es una biblioteca de optimización automática de hiperparámetros, utilizada para mejorar el rendimiento de los modelos de machine learning. Funciona de manera eficiente utilizando algoritmos avanzados de optimización como el algoritmo de búsqueda de bayesiana para explorar y encontrar los mejores valores de los hiperparámetros."
      ],
      "metadata": {
        "id": "GBbUOPFIFDRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "# Instalar optuna\n",
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8XSIzATAgiQ",
        "outputId": "295dca87-c4a1-4b89-d396-50e9094d0ae7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.14.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.37)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "import optuna\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Definimos la función de objetivo para Optuna\n",
        "def objective(trial):\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=trial.suggest_int('n_estimators', 50, 200),\n",
        "        max_depth=trial.suggest_int('max_depth', 5, 20),\n",
        "        min_samples_split=trial.suggest_int('min_samples_split', 2, 10),\n",
        "        min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 5),\n",
        "        bootstrap=trial.suggest_categorical('bootstrap', [True, False]),\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_val_scaled)\n",
        "    return mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "# Crear un estudio y optimizar\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Imprimir los mejores parámetros y el MAE\n",
        "print(f\"Mejores parámetros: {study.best_params}\")\n",
        "print(f\"Mejor MAE: {study.best_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwerV8QhAZ-V",
        "outputId": "72e47653-9382-499a-da1e-22975c023409"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-14 11:28:19,219] A new study created in memory with name: no-name-8846c479-4ef4-4394-88f1-42c947dfc871\n",
            "[I 2025-02-14 11:28:22,477] Trial 0 finished with value: 14.55389123256209 and parameters: {'n_estimators': 125, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 0 with value: 14.55389123256209.\n",
            "[I 2025-02-14 11:28:29,115] Trial 1 finished with value: 14.672885817712494 and parameters: {'n_estimators': 169, 'max_depth': 18, 'min_samples_split': 8, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 0 with value: 14.55389123256209.\n",
            "[I 2025-02-14 11:28:32,824] Trial 2 finished with value: 15.539638127853877 and parameters: {'n_estimators': 75, 'max_depth': 17, 'min_samples_split': 8, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 0 with value: 14.55389123256209.\n",
            "[I 2025-02-14 11:28:37,017] Trial 3 finished with value: 15.31579755379983 and parameters: {'n_estimators': 143, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 0 with value: 14.55389123256209.\n",
            "[I 2025-02-14 11:28:44,639] Trial 4 finished with value: 14.761325425645488 and parameters: {'n_estimators': 196, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 0 with value: 14.55389123256209.\n",
            "[I 2025-02-14 11:28:49,484] Trial 5 finished with value: 17.317686455012165 and parameters: {'n_estimators': 95, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 1, 'bootstrap': False}. Best is trial 0 with value: 14.55389123256209.\n",
            "[I 2025-02-14 11:28:51,405] Trial 6 finished with value: 15.806479778912978 and parameters: {'n_estimators': 65, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 0 with value: 14.55389123256209.\n",
            "[I 2025-02-14 11:28:56,190] Trial 7 finished with value: 15.044805826720507 and parameters: {'n_estimators': 193, 'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 0 with value: 14.55389123256209.\n",
            "[I 2025-02-14 11:28:58,223] Trial 8 finished with value: 14.513331537608986 and parameters: {'n_estimators': 119, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 8 with value: 14.513331537608986.\n",
            "[I 2025-02-14 11:29:00,041] Trial 9 finished with value: 15.31579755379983 and parameters: {'n_estimators': 143, 'max_depth': 7, 'min_samples_split': 3, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 8 with value: 14.513331537608986.\n",
            "[I 2025-02-14 11:29:01,146] Trial 10 finished with value: 16.430253761158482 and parameters: {'n_estimators': 105, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 8 with value: 14.513331537608986.\n",
            "[I 2025-02-14 11:29:03,764] Trial 11 finished with value: 14.611389481114111 and parameters: {'n_estimators': 117, 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 8 with value: 14.513331537608986.\n",
            "[I 2025-02-14 11:29:06,684] Trial 12 finished with value: 14.510076462111126 and parameters: {'n_estimators': 140, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 12 with value: 14.510076462111126.\n",
            "[I 2025-02-14 11:29:09,289] Trial 13 finished with value: 14.618685681197118 and parameters: {'n_estimators': 149, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 12 with value: 14.510076462111126.\n",
            "[I 2025-02-14 11:29:10,916] Trial 14 finished with value: 14.696642990027092 and parameters: {'n_estimators': 90, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 12 with value: 14.510076462111126.\n",
            "[I 2025-02-14 11:29:13,415] Trial 15 finished with value: 14.75080512341765 and parameters: {'n_estimators': 167, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 12 with value: 14.510076462111126.\n",
            "[I 2025-02-14 11:29:15,295] Trial 16 finished with value: 15.086869186932477 and parameters: {'n_estimators': 115, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 12 with value: 14.510076462111126.\n",
            "[I 2025-02-14 11:29:18,723] Trial 17 finished with value: 14.762932749715587 and parameters: {'n_estimators': 159, 'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 12 with value: 14.510076462111126.\n",
            "[I 2025-02-14 11:29:22,628] Trial 18 finished with value: 15.689967662142962 and parameters: {'n_estimators': 133, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 12 with value: 14.510076462111126.\n",
            "[I 2025-02-14 11:29:23,202] Trial 19 finished with value: 16.447552254337328 and parameters: {'n_estimators': 53, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 12 with value: 14.510076462111126.\n",
            "[I 2025-02-14 11:29:26,212] Trial 20 finished with value: 14.77451137370464 and parameters: {'n_estimators': 181, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 12 with value: 14.510076462111126.\n",
            "[I 2025-02-14 11:29:28,725] Trial 21 finished with value: 14.59548886589757 and parameters: {'n_estimators': 127, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 12 with value: 14.510076462111126.\n",
            "[I 2025-02-14 11:29:31,856] Trial 22 finished with value: 14.486501798537475 and parameters: {'n_estimators': 130, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 22 with value: 14.486501798537475.\n",
            "[I 2025-02-14 11:29:33,877] Trial 23 finished with value: 14.734953520424101 and parameters: {'n_estimators': 102, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 22 with value: 14.486501798537475.\n",
            "[I 2025-02-14 11:29:36,380] Trial 24 finished with value: 14.519573848510602 and parameters: {'n_estimators': 138, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 22 with value: 14.486501798537475.\n",
            "[I 2025-02-14 11:29:39,896] Trial 25 finished with value: 14.710566121215127 and parameters: {'n_estimators': 155, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 22 with value: 14.486501798537475.\n",
            "[I 2025-02-14 11:29:43,047] Trial 26 finished with value: 16.177346156801345 and parameters: {'n_estimators': 115, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 22 with value: 14.486501798537475.\n",
            "[I 2025-02-14 11:29:44,806] Trial 27 finished with value: 14.490620441145772 and parameters: {'n_estimators': 82, 'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 22 with value: 14.486501798537475.\n",
            "[I 2025-02-14 11:29:46,486] Trial 28 finished with value: 14.760251233423489 and parameters: {'n_estimators': 83, 'max_depth': 13, 'min_samples_split': 5, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 22 with value: 14.486501798537475.\n",
            "[I 2025-02-14 11:29:48,885] Trial 29 finished with value: 14.695480100209968 and parameters: {'n_estimators': 131, 'max_depth': 16, 'min_samples_split': 7, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 22 with value: 14.486501798537475.\n",
            "[I 2025-02-14 11:29:49,851] Trial 30 finished with value: 14.750486516522841 and parameters: {'n_estimators': 51, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 22 with value: 14.486501798537475.\n",
            "[I 2025-02-14 11:29:52,058] Trial 31 finished with value: 14.462419203807919 and parameters: {'n_estimators': 121, 'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 31 with value: 14.462419203807919.\n",
            "[I 2025-02-14 11:29:53,430] Trial 32 finished with value: 14.584764309497269 and parameters: {'n_estimators': 72, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 31 with value: 14.462419203807919.\n",
            "[I 2025-02-14 11:29:56,200] Trial 33 finished with value: 14.898605006475455 and parameters: {'n_estimators': 124, 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 31 with value: 14.462419203807919.\n",
            "[I 2025-02-14 11:29:58,228] Trial 34 finished with value: 14.651073610865309 and parameters: {'n_estimators': 108, 'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 31 with value: 14.462419203807919.\n",
            "[I 2025-02-14 11:30:02,058] Trial 35 finished with value: 14.737525168792834 and parameters: {'n_estimators': 165, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 31 with value: 14.462419203807919.\n",
            "[I 2025-02-14 11:30:06,485] Trial 36 finished with value: 15.776124761818926 and parameters: {'n_estimators': 152, 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 31 with value: 14.462419203807919.\n",
            "[I 2025-02-14 11:30:10,670] Trial 37 finished with value: 14.584539051388033 and parameters: {'n_estimators': 177, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 31 with value: 14.462419203807919.\n",
            "[I 2025-02-14 11:30:14,010] Trial 38 finished with value: 15.159292546247991 and parameters: {'n_estimators': 92, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 31 with value: 14.462419203807919.\n",
            "[I 2025-02-14 11:30:16,817] Trial 39 finished with value: 14.479563940125146 and parameters: {'n_estimators': 137, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 31 with value: 14.462419203807919.\n",
            "[I 2025-02-14 11:30:18,633] Trial 40 finished with value: 14.744491148422615 and parameters: {'n_estimators': 77, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 31 with value: 14.462419203807919.\n",
            "[I 2025-02-14 11:30:22,228] Trial 41 finished with value: 14.521144240598328 and parameters: {'n_estimators': 142, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 31 with value: 14.462419203807919.\n",
            "[I 2025-02-14 11:30:24,861] Trial 42 finished with value: 14.472088476456818 and parameters: {'n_estimators': 139, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 31 with value: 14.462419203807919.\n",
            "[I 2025-02-14 11:30:27,370] Trial 43 finished with value: 14.524772908206549 and parameters: {'n_estimators': 124, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 31 with value: 14.462419203807919.\n",
            "[I 2025-02-14 11:30:29,969] Trial 44 finished with value: 14.400612660852017 and parameters: {'n_estimators': 134, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 44 with value: 14.400612660852017.\n",
            "[I 2025-02-14 11:30:32,490] Trial 45 finished with value: 14.513400137461023 and parameters: {'n_estimators': 135, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 44 with value: 14.400612660852017.\n",
            "[I 2025-02-14 11:30:37,732] Trial 46 finished with value: 17.551185462158205 and parameters: {'n_estimators': 145, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 1, 'bootstrap': False}. Best is trial 44 with value: 14.400612660852017.\n",
            "[I 2025-02-14 11:30:39,943] Trial 47 finished with value: 14.65045507294904 and parameters: {'n_estimators': 110, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 44 with value: 14.400612660852017.\n",
            "[I 2025-02-14 11:30:41,993] Trial 48 finished with value: 14.603413365172566 and parameters: {'n_estimators': 100, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 44 with value: 14.400612660852017.\n",
            "[I 2025-02-14 11:30:44,594] Trial 49 finished with value: 14.548443887560445 and parameters: {'n_estimators': 127, 'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 44 with value: 14.400612660852017.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores parámetros: {'n_estimators': 134, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}\n",
            "Mejor MAE: 14.400612660852017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pipeline con RandomForest"
      ],
      "metadata": {
        "id": "AxnyPlkhFcIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Pipeline con imputación, escalado y RandomForest\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Entrenamiento y evaluación\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_val)\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "print(\"MAE con pipeline: \", mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I4pRCeNBlGq",
        "outputId": "b769aab3-cd12-4460-b4d3-521d945d1172"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE con pipeline:  14.61359589041096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargar fichero"
      ],
      "metadata": {
        "id": "sWpW-97BFeEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVAN FALCON MONZON\n",
        "from google.colab import files\n",
        "\n",
        "# Asegúrate de que el archivo con el nombre correcto esté en el entorno de Colab\n",
        "files.download('submission.csv')  # Cambia el nombre si es necesario"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-svd-11xBvlq",
        "outputId": "0e5cd75a-efe8-47e4-f5e4-af9799fa5468"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ac1e1756-f473-414d-bc93-ae83b6eccabd\", \"submission.csv\", 5675)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones de los nuevos modelos y predicciones:"
      ],
      "metadata": {
        "id": "4gd_CZK9Ji96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Random Forest (después de limpiar y transformar datos):\n",
        "* El MAE es 14.61, lo que indica un desempeño moderado tras la limpieza y transformación de los datos.\n",
        "\n",
        "2. GridSearchCV:\n",
        "* Mejores parámetros: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}.\n",
        "* Mejor MAE: 13.50. Este enfoque de búsqueda exhaustiva mejora el modelo, logrando un MAE más bajo, indicando una optimización efectiva de los hiperparámetros.\n",
        "\n",
        "3. RandomizedSearchCV:\n",
        "* Mejores parámetros: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 20, 'bootstrap': True}.\n",
        "* Mejor MAE: 13.86. La búsqueda aleatoria también mostró buenos resultados, pero el MAE es ligeramente peor que el obtenido con GridSearchCV.\n",
        "\n",
        "4. Evaluación de los resultados:\n",
        "* Después de la optimización de los hiperparámetros, el MAE sube a 14.75, lo que sugiere que aunque hubo mejoras, los resultados no son consistentemente mejores que los obtenidos por otros enfoques.\n",
        "\n",
        "5. XGBoost:\n",
        "* MAE: 14.38. Aunque XGBoost es una técnica robusta, el desempeño es ligeramente mejor que el Random Forest estándar pero aún no mejora significativamente los mejores resultados previos.\n",
        "\n",
        "6. Optuna:\n",
        "* Mejores parámetros: {'n_estimators': 169, 'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}.\n",
        "* Mejor MAE: 14.38. Optuna mostró una ligera mejora respecto al Random Forest y XGBoost, pero no superó los mejores resultados de GridSearchCV.\n",
        "\n",
        "7. Pipeline con RandomForest:\n",
        "* MAE: 14.61, similar al rendimiento inicial sin pipeline, lo que indica que no se logró una mejora significativa con esta implementación."
      ],
      "metadata": {
        "id": "EGqmSaU_JkRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Resumen:**\n",
        "En general, el enfoque de GridSearchCV ha mostrado el mejor desempeño con el menor MAE (13.50), mientras que el uso de XGBoost y Optuna también ha sido prometedor, aunque sin superar el rendimiento de GridSearchCV. La optimización de los hiperparámetros es crucial para mejorar el desempeño del modelo, pero la diferencia entre los enfoques no ha sido sustancial."
      ],
      "metadata": {
        "id": "D-QLFvMrKCAk"
      }
    }
  ]
}